{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r ../src/orchestration/requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import pendulum\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.train.xgboost import XGBoostTrainer\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "\n",
    "\n",
    "ray.init(\"ray://localhost:10001\", namespace=\"experiment-1\", log_to_driver=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "TRAINING_CONFIG = {\n",
    "    \"model_path\": \"model-checkpoints/final-model/xgb_model\",\n",
    "    \"test_size\": 0.3,\n",
    "    \"num_workers\": 1,\n",
    "    \"resources_per_worker\": {\"CPU\": 4},\n",
    "    \"use_gpu\": False,\n",
    "    \"num_boost_round\": 1,\n",
    "}\n",
    "\n",
    "# XGBoost Parameters\n",
    "XGBOOST_PARAMS = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": [\"logloss\", \"error\", \"rmse\", \"mae\", \"auc\"],\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"max_depth\": 1,\n",
    "    \"eta\": 0.3,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "}\n",
    "\n",
    "# Feature Configuration\n",
    "FEATURE_COLUMNS = [\n",
    "    \"brand\",\n",
    "    \"price\",\n",
    "    \"event_weekday\",\n",
    "    \"category_code_level1\",\n",
    "    \"category_code_level2\",\n",
    "    \"activity_count\",\n",
    "    \"is_purchased\",\n",
    "]\n",
    "\n",
    "CATEGORICAL_COLUMNS = [\n",
    "    \"brand\",\n",
    "    \"event_weekday\",\n",
    "    \"category_code_level1\",\n",
    "    \"category_code_level2\",\n",
    "]\n",
    "\n",
    "# DAG Configuration\n",
    "DEFAULT_ARGS = {\n",
    "    \"owner\": \"airflow\",\n",
    "    \"depends_on_past\": False,\n",
    "    \"email_on_failure\": True,\n",
    "    \"email_on_retry\": False,\n",
    "    \"retries\": 2,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "    \"retry_exponential_backoff\": True,\n",
    "    \"max_retry_delay\": timedelta(minutes=30),\n",
    "    \"execution_timeout\": timedelta(hours=2),\n",
    "    \"start_date\": pendulum.datetime(2024, 1, 1, tz=\"UTC\"),\n",
    "}\n",
    "\n",
    "# Tune Configuration\n",
    "TUNE_CONFIG = {\n",
    "    \"model_path\": \"model-checkpoints/hyperparameter-tuning/xgb_model\",\n",
    "    \"num_trials\": 4,  # Number of trials for hyperparameter search\n",
    "    \"max_epochs\": 4,  # Maximum epochs per trial\n",
    "    \"grace_period\": 4,  # Minimum epochs before pruning\n",
    "    \"mlflow_tracking_uri\": os.getenv(\"MLFLOW_TRACKING_URI\", \"http://0.0.0.0:5001\"),\n",
    "}\n",
    "\n",
    "# Tune Search Space\n",
    "TUNE_SEARCH_SPACE = {\n",
    "    \"max_depth\": tune.randint(3, 5),\n",
    "    \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"min_child_weight\": tune.choice([1, 2, 3, 4, 5]),\n",
    "    \"subsample\": tune.uniform(0.5, 1.0),\n",
    "    \"colsample_bytree\": tune.uniform(0.5, 1.0),\n",
    "    \"gamma\": tune.uniform(0, 1),\n",
    "}\n",
    "\n",
    "# Model Configuration\n",
    "MODEL_NAME = \"purchase_prediction_model\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPipeline:\n",
    "    def __init__(self):\n",
    "        self.postgres_conn = \"postgresql://dwh:dwh@localhost:5433/dwh\"\n",
    "        mlflow.set_tracking_uri(TUNE_CONFIG[\"mlflow_tracking_uri\"])\n",
    "\n",
    "    def load_training_data(self) -> Dict[str, List[Dict]]:\n",
    "        df = pd.read_sql(\n",
    "            \"SELECT * FROM dwh.vw_ml_purchase_prediction\", self.postgres_conn\n",
    "        )\n",
    "\n",
    "        print(f\"Loaded {len(df)} rows of data\")\n",
    "\n",
    "        # Data preprocessing\n",
    "        df[\"price\"] = df[\"price\"].astype(float)\n",
    "\n",
    "        # Create category mappings\n",
    "        category_mappings = {}\n",
    "        for col in CATEGORICAL_COLUMNS:\n",
    "            unique_values = df[col].dropna().unique()\n",
    "            category_mapping = {\n",
    "                val: idx for idx, val in enumerate(sorted(unique_values))\n",
    "            }\n",
    "            category_mappings[col] = category_mapping\n",
    "            df[col] = df[col].map(category_mapping).fillna(-1)\n",
    "\n",
    "        return {\n",
    "            \"data\": df.to_dict(orient=\"records\"),\n",
    "            \"category_mappings\": category_mappings,\n",
    "        }\n",
    "\n",
    "    def tune_hyperparameters(self, data: dict) -> dict:\n",
    "        experiment_name = f\"experiment-1_xgb_tune_{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "        mlflow.create_experiment(experiment_name)\n",
    "\n",
    "        df = pd.DataFrame(data[\"data\"])\n",
    "        dataset = ray.data.from_pandas(df)\n",
    "\n",
    "        def train_xgboost(config):\n",
    "            training_params = {**XGBOOST_PARAMS, **config}\n",
    "            trainer = XGBoostTrainer(\n",
    "                label_column=\"is_purchased\",\n",
    "                num_boost_round=TRAINING_CONFIG[\"num_boost_round\"],\n",
    "                params=training_params,\n",
    "                datasets={\"train\": dataset},\n",
    "            )\n",
    "            results = trainer.fit()\n",
    "            ray.train.report(results.metrics)\n",
    "\n",
    "        tuner = ray.tune.run(\n",
    "            train_xgboost,\n",
    "            config=TUNE_SEARCH_SPACE,\n",
    "            num_samples=TUNE_CONFIG[\"num_trials\"],\n",
    "            scheduler=ASHAScheduler(\n",
    "                metric=\"train_rmse\",\n",
    "                mode=\"min\",\n",
    "            ),\n",
    "            search_alg=OptunaSearch(\n",
    "                metric=\"train_rmse\",\n",
    "                mode=\"min\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        best_trial = tuner.get_best_trial(\"train_rmse\", \"min\")\n",
    "        return {\n",
    "            \"best_config\": best_trial.config,\n",
    "            \"best_metrics\": best_trial.last_result,\n",
    "        }\n",
    "\n",
    "    def train_final_model(self, data: dict, best_params: dict) -> dict:\n",
    "        experiment_name = f\"xgb_final_{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "        mlflow.create_experiment(experiment_name)\n",
    "\n",
    "        with mlflow.start_run() as run:\n",
    "            model_params = {**XGBOOST_PARAMS, **best_params[\"best_config\"]}\n",
    "            df = pd.DataFrame(data[\"data\"])\n",
    "            dataset = ray.data.from_pandas(df)\n",
    "\n",
    "            trainer = XGBoostTrainer(\n",
    "                label_column=\"is_purchased\",\n",
    "                num_boost_round=TRAINING_CONFIG[\"num_boost_round\"],\n",
    "                params=model_params,\n",
    "                datasets={\"train\": dataset},\n",
    "            )\n",
    "\n",
    "            result = trainer.fit()\n",
    "\n",
    "            # Log model and metrics\n",
    "            mlflow.xgboost.log_model(\n",
    "                result.checkpoint.get_model(),\n",
    "                \"model\",\n",
    "                registered_model_name=MODEL_NAME,\n",
    "            )\n",
    "\n",
    "            # Log category mappings if available\n",
    "            if \"category_mappings\" in data:\n",
    "                mlflow.log_dict(data[\"category_mappings\"], \"category_mappings.json\")\n",
    "\n",
    "            return {\n",
    "                \"metrics\": result.metrics,\n",
    "                \"checkpoint_path\": result.checkpoint.path,\n",
    "                \"mlflow_run_id\": run.info.run_id,\n",
    "                \"mlflow_model_uri\": f\"models:/{MODEL_NAME}/Staging\",\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ModelPipeline()\n",
    "\n",
    "# Load and preprocess data\n",
    "data = pipeline.load_training_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = f\"experiment-1_xgb_tune_{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "mlflow.create_experiment(experiment_name)\n",
    "\n",
    "df = pd.DataFrame(data[\"data\"])\n",
    "dataset = ray.data.from_pandas(df)\n",
    "\n",
    "dataset.show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_xgboost(config):\n",
    "    training_params = {**XGBOOST_PARAMS, **config}\n",
    "    trainer = XGBoostTrainer(\n",
    "        label_column=\"is_purchased\",\n",
    "        num_boost_round=TRAINING_CONFIG[\"num_boost_round\"],\n",
    "        params=training_params,\n",
    "        datasets={\"train\": dataset},\n",
    "    )\n",
    "    results = trainer.fit()\n",
    "    ray.train.report(results.metrics)\n",
    "\n",
    "tuner = ray.tune.run(\n",
    "    train_xgboost,\n",
    "    config=TUNE_SEARCH_SPACE,\n",
    "    num_samples=TUNE_CONFIG[\"num_trials\"],\n",
    "    scheduler=ASHAScheduler(\n",
    "        metric=\"train_rmse\",\n",
    "        mode=\"min\",\n",
    "    ),\n",
    "    search_alg=OptunaSearch(\n",
    "        metric=\"train_rmse\",\n",
    "        mode=\"min\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "best_trial = tuner.get_best_trial(\"train_rmse\", \"min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train final model\n",
    "results = pipeline.train_final_model(data, best_params)\n",
    "\n",
    "print(\"Training completed successfully!\")\n",
    "print(f\"Final metrics: {results['metrics']}\")\n",
    "print(f\"Model URI: {results['mlflow_model_uri']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
